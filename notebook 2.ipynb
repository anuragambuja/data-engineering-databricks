{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d84644-8b99-4dc4-bbb2-0ad509ee5c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/derar-alhussein/Databricks-Certified-Data-Engineer-Professional/main/Includes/images/orders.png\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07bec64-664a-4a92-8c96-0f382abe5bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Copy-Datasets-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488f06a9-0bbe-4e06-80d6-3b4c88de3958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# files = dbutils.fs.ls(f\"{dataset_bookstore}/kafka-raw\")\n",
    "# display(files)\n",
    "\n",
    "# df_raw = spark.read.json(f\"{dataset_bookstore}/kafka-raw\")\n",
    "# display(df_raw)\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def process_bronze():\n",
    "  \n",
    "#     schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "#     query = (spark.readStream\n",
    "#                         .format(\"cloudFiles\")\n",
    "#                         .option(\"cloudFiles.format\", \"json\")\n",
    "#                         .schema(schema)\n",
    "#                         .load(f\"{dataset_bookstore}/kafka-raw\")\n",
    "#                         .withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))  \n",
    "#                         .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "#                   .writeStream\n",
    "#                       .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/bronze\")\n",
    "#                       .option(\"mergeSchema\", True) # enables schema evolution\n",
    "#                       .partitionBy(\"topic\", \"year_month\")\n",
    "#                       .trigger(availableNow=True)\n",
    "#                       .table(\"bronze\"))\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "\n",
    "# process_bronze()\n",
    "\n",
    "# batch_df = spark.table(\"bronze\")\n",
    "# display(batch_df)\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM bronze;\n",
    "# -- SELECT DISTINCT(topic) FROM bronze;\n",
    "# SELECT COUNT(*) FROM bronze\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# process_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb72a84-efe9-4752-9b68-58109541ac38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT cast(key AS STRING), cast(value AS STRING) FROM bronze LIMIT 20;\n",
    "\n",
    "# SELECT v.*\n",
    "# FROM (\n",
    "#   SELECT from_json(cast(value AS STRING), \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\") v\n",
    "#   FROM bronze\n",
    "#   WHERE topic = \"orders\")\n",
    "\n",
    "# (spark.readStream\n",
    "#       .table(\"bronze\")\n",
    "#       .createOrReplaceTempView(\"bronze_tmp\"))\n",
    "\n",
    "# %sql\n",
    "# CREATE OR REPLACE TEMPORARY VIEW orders_silver_tmp AS\n",
    "#   SELECT v.*\n",
    "#   FROM (\n",
    "#     SELECT from_json(cast(value AS STRING), \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\") v\n",
    "#     FROM bronze_tmp\n",
    "#     WHERE topic = \"orders\")\n",
    "\n",
    "# query = (spark.table(\"orders_silver_tmp\")\n",
    "#                .writeStream\n",
    "#                .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#                .trigger(availableNow=True)\n",
    "#                .table(\"orders_silver\"))\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "# query = (spark.readStream.table(\"bronze\")\n",
    "#         .filter(\"topic = 'orders'\")\n",
    "#         .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#         .select(\"v.*\")\n",
    "#      .writeStream\n",
    "#         .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#         .trigger(availableNow=True)\n",
    "#         .table(\"orders_silver\"))\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM orders_silver;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45da265-df20-48bf-8ec1-bc4dd4f1a91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE orders_silver ADD CONSTRAINT timestamp_within_range CHECK (order_timestamp >= '2020-01-01');\n",
    "# DESCRIBE EXTENDED orders_silver;\n",
    "\n",
    "# INSERT INTO orders_silver\n",
    "# VALUES ('1', '2022-02-01 00:00:00.000', 'C00001', 0, 0, NULL),\n",
    "#        ('2', '2019-05-01 00:00:00.000', 'C00001', 0, 0, NULL),\n",
    "#        ('3', '2023-01-01 00:00:00.000', 'C00001', 0, 0, NULL);\n",
    "\n",
    "# SELECT *\n",
    "# FROM orders_silver\n",
    "# WHERE order_id IN ('1', '2', '3');\n",
    "\n",
    "# ALTER TABLE orders_silver ADD CONSTRAINT valid_quantity CHECK (quantity > 0);\n",
    "\n",
    "# DESCRIBE EXTENDED orders_silver;\n",
    "\n",
    "# SELECT *\n",
    "# FROM orders_silver\n",
    "# where quantity <= 0;\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "# query = (spark.readStream.table(\"bronze\")\n",
    "#         .filter(\"topic = 'orders'\")\n",
    "#         .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#         .select(\"v.*\")\n",
    "#         .filter(\"quantity > 0\")\n",
    "#      .writeStream\n",
    "#         .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#         .trigger(availableNow=True)\n",
    "#         .table(\"orders_silver\"))\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# %sql\n",
    "# ALTER TABLE orders_silver DROP CONSTRAINT timestamp_within_range;\n",
    "# DESCRIBE EXTENDED orders_silver;\n",
    "# DROP TABLE orders_silver;\n",
    "\n",
    "# dbutils.fs.rm(\"dbfs:/mnt/demo_pro/checkpoints/orders_silver\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d41d8d9-2ad4-4b7b-9141-e6f5875f9a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (spark.read\n",
    "#       .table(\"bronze\")\n",
    "#       .filter(\"topic = 'orders'\")\n",
    "#       .count()\n",
    "# )\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "# batch_total = (spark.read\n",
    "#                       .table(\"bronze\")\n",
    "#                       .filter(\"topic = 'orders'\")\n",
    "#                       .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#                       .select(\"v.*\")\n",
    "#                       .dropDuplicates([\"order_id\", \"order_timestamp\"])\n",
    "#                       .count()\n",
    "#                 )\n",
    "\n",
    "# print(batch_total)\n",
    "\n",
    "# deduped_df = (spark.readStream\n",
    "#                    .table(\"bronze\")\n",
    "#                    .filter(\"topic = 'orders'\")\n",
    "#                    .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#                    .select(\"v.*\")\n",
    "#                    .withWatermark(\"order_timestamp\", \"30 seconds\") ## ensures no duplicates in 30 secornd microbatch\n",
    "#                    .dropDuplicates([\"order_id\", \"order_timestamp\"]))\n",
    "\n",
    "# to make sure duplicates are not there in table\n",
    "# def upsert_data(microBatchDF, batch):\n",
    "#     microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "#     sql_query = \"\"\"\n",
    "#       MERGE INTO orders_silver a\n",
    "#       USING orders_microbatch b\n",
    "#       ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "#       WHEN NOT MATCHED THEN INSERT *\n",
    "#     \"\"\"\n",
    "    \n",
    "#     microBatchDF.sparkSession.sql(sql_query)\n",
    "#     #microBatchDF._jdf.sparkSession().sql(sql_query) # for runtime version below 10.5\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS orders_silver\n",
    "# (order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>);\n",
    "\n",
    "# query = (deduped_df.writeStream\n",
    "#                    .foreachBatch(upsert_data)\n",
    "#                    .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#                    .trigger(availableNow=True)\n",
    "#                    .start())\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "# streaming_total = spark.read.table(\"orders_silver\").count()\n",
    "# print(f\"batch total: {batch_total}\")\n",
    "# print(f\"streaming total: {streaming_total}\") # values match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67c7a5a-2d5f-454d-81b9-d7e0ef8bfa3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def type2_upsert(microBatchDF, batch):\n",
    "#     microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    \n",
    "#     sql_query = \"\"\"\n",
    "#         MERGE INTO books_silver\n",
    "#         USING (\n",
    "#             SELECT updates.book_id as merge_key, updates.*\n",
    "#             FROM updates\n",
    "\n",
    "#             UNION ALL\n",
    "\n",
    "#             SELECT NULL as merge_key, updates.*\n",
    "#             FROM updates\n",
    "#             JOIN books_silver ON updates.book_id = books_silver.book_id\n",
    "#             WHERE books_silver.current = true AND updates.price <> books_silver.price\n",
    "#           ) staged_updates\n",
    "#         ON books_silver.book_id = merge_key \n",
    "#         WHEN MATCHED AND books_silver.current = true AND books_silver.price <> staged_updates.price THEN\n",
    "#           UPDATE SET current = false, end_date = staged_updates.updated\n",
    "#         WHEN NOT MATCHED THEN\n",
    "#           INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "#           VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     microBatchDF.sparkSession.sql(sql_query)\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS books_silver\n",
    "# (book_id STRING, title STRING, author STRING, price DOUBLE, current BOOLEAN, effective_date TIMESTAMP, end_date TIMESTAMP)\n",
    "\n",
    "\n",
    "# def process_books():\n",
    "#     schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    " \n",
    "#     query = (spark.readStream\n",
    "#                     .table(\"bronze\")\n",
    "#                     .filter(\"topic = 'books'\")\n",
    "#                     .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                     .select(\"v.*\")\n",
    "#                  .writeStream\n",
    "#                     .foreachBatch(type2_upsert)\n",
    "#                     .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/books_silver\")\n",
    "#                     .trigger(availableNow=True)\n",
    "#                     .start()\n",
    "#             )\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "    \n",
    "# process_books()\n",
    "\n",
    "# books_df = spark.read.table(\"books_silver\").orderBy(\"book_id\", \"effective_date\")\n",
    "# display(books_df)\n",
    "\n",
    "# bookstore.load_books_updates()\n",
    "# bookstore.process_bronze()\n",
    "# process_books()\n",
    "\n",
    "# books_df = spark.read.table(\"books_silver\").orderBy(\"book_id\", \"effective_date\")\n",
    "# display(books_df)\n",
    "\n",
    "\n",
    "# %sql\n",
    "# CREATE OR REPLACE TABLE current_books\n",
    "# AS SELECT book_id, title, author, price\n",
    "#    FROM books_silver\n",
    "#    WHERE current IS TRUE;\n",
    "\n",
    "# SELECT *\n",
    "# FROM current_books\n",
    "# ORDER BY book_id;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc90297-a79c-4e0f-a202-153d375f6cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "# customers_df = (spark.table(\"bronze\")\n",
    "#                  .filter(\"topic = 'customers'\")\n",
    "#                  .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                  .select(\"v.*\")\n",
    "#                  .filter(F.col(\"row_status\").isin([\"insert\", \"update\"])))\n",
    "# display(customers_df)\n",
    "\n",
    "# from pyspark.sql.window import Window\n",
    "# window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "# ranked_df = (customers_df.withColumn(\"rank\", F.rank().over(window))\n",
    "#                           .filter(\"rank == 1\")\n",
    "#                           .drop(\"rank\"))\n",
    "# display(ranked_df)\n",
    "\n",
    "# This will throw an exception because non-time-based window operations are not supported on streaming DataFrames.\n",
    "# ranked_df = (spark.readStream\n",
    "#                    .table(\"bronze\")\n",
    "#                    .filter(\"topic = 'customers'\")\n",
    "#                    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                    .select(\"v.*\")\n",
    "#                    .filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "#                    .withColumn(\"rank\", F.rank().over(window))\n",
    "#                    .filter(\"rank == 1\")\n",
    "#                    .drop(\"rank\")\n",
    "#              )\n",
    "# display(ranked_df)\n",
    "\n",
    "\n",
    "# from pyspark.sql.window import Window\n",
    "# def batch_upsert(microBatchDF, batchId):\n",
    "#     window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "    \n",
    "#     (microBatchDF.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "#                  .withColumn(\"rank\", F.rank().over(window))\n",
    "#                  .filter(\"rank == 1\")\n",
    "#                  .drop(\"rank\")\n",
    "#                  .createOrReplaceTempView(\"ranked_updates\"))\n",
    "    \n",
    "#     query = \"\"\"\n",
    "#         MERGE INTO customers_silver c\n",
    "#         USING ranked_updates r\n",
    "#         ON c.customer_id=r.customer_id\n",
    "#             WHEN MATCHED AND c.row_time < r.row_time\n",
    "#               THEN UPDATE SET *\n",
    "#             WHEN NOT MATCHED\n",
    "#               THEN INSERT *\n",
    "#     \"\"\"\n",
    "#     microBatchDF.sparkSession.sql(query)\n",
    "\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS customers_silver\n",
    "# (customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country STRING, row_time TIMESTAMP)\n",
    "\n",
    "# df_country_lookup = spark.read.json(f\"{dataset_bookstore}/country_lookup\")\n",
    "# display(df_country_lookup)\n",
    "\n",
    "# query = (spark.readStream\n",
    "#                   .table(\"bronze\")\n",
    "#                   .filter(\"topic = 'customers'\")\n",
    "#                   .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                   .select(\"v.*\")\n",
    "#                   .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\") , \"inner\")\n",
    "#                .writeStream\n",
    "#                   .foreachBatch(batch_upsert)\n",
    "#                   .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/customers_silver\")\n",
    "#                   .trigger(availableNow=True)\n",
    "#                   .start()\n",
    "#           )\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# count = spark.table(\"customers_silver\").count()\n",
    "# expected_count = spark.table(\"customers_silver\").select(\"customer_id\").distinct().count()\n",
    "# assert count == expected_count\n",
    "# print(\"Unit test passed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0364fb5-3bf4-42ad-980a-3b2bd4630a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE customers_silver SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "# DESCRIBE TABLE EXTENDED customers_silver;\n",
    "# DESCRIBE HISTORY customers_silver;\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM table_changes(\"customers_silver\", 2);\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "\n",
    "# cdf_df = (spark.readStream\n",
    "#                .format(\"delta\")\n",
    "#                .option(\"readChangeData\", True)\n",
    "#                .option(\"startingVersion\", 2)\n",
    "#                .table(\"customers_silver\"))\n",
    "\n",
    "# display(cdf_df)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver\")\n",
    "# display(files)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver/_change_data\")\n",
    "# display(files)\n",
    "\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "\n",
    "# cdf_df = (spark.readStream\n",
    "#                .format(\"delta\")\n",
    "#                .option(\"readChangeData\", True)\n",
    "#                .option(\"startingVersion\", 2)\n",
    "#                .table(\"customers_silver\"))\n",
    "\n",
    "# display(cdf_df)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver\")\n",
    "# display(files)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver/_change_data\")\n",
    "# display(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd9745b-4608-4fab-a38f-9c6e43a7a572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# def batch_upsert(microBatchDF, batchId):\n",
    "#     window = Window.partitionBy(\"order_id\", \"customer_id\").orderBy(F.col(\"_commit_timestamp\").desc())\n",
    "    \n",
    "#     (microBatchDF.filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\"]))\n",
    "#                  .withColumn(\"rank\", F.rank().over(window))\n",
    "#                  .filter(\"rank = 1\")\n",
    "#                  .drop(\"rank\", \"_change_type\", \"_commit_version\")\n",
    "#                  .withColumnRenamed(\"_commit_timestamp\", \"processed_timestamp\")\n",
    "#                  .createOrReplaceTempView(\"ranked_updates\"))\n",
    "    \n",
    "#     query = \"\"\"\n",
    "#         MERGE INTO customers_orders c\n",
    "#         USING ranked_updates r\n",
    "#         ON c.order_id=r.order_id AND c.customer_id=r.customer_id\n",
    "#             WHEN MATCHED AND c.processed_timestamp < r.processed_timestamp\n",
    "#               THEN UPDATE SET *\n",
    "#             WHEN NOT MATCHED\n",
    "#               THEN INSERT *\n",
    "#     \"\"\"\n",
    "    \n",
    "#     microBatchDF.sparkSession.sql(query)\n",
    "\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS customers_orders\n",
    "# (order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country STRING, row_time TIMESTAMP, processed_timestamp TIMESTAMP)\n",
    "\n",
    "\n",
    "# def process_customers_orders():\n",
    "#     orders_df = spark.readStream.table(\"orders_silver\")\n",
    "    \n",
    "#     cdf_customers_df = (spark.readStream\n",
    "#                              .option(\"readChangeData\", True)\n",
    "#                              .option(\"startingVersion\", 2)\n",
    "#                              .table(\"customers_silver\")\n",
    "#                        )\n",
    "\n",
    "#     query = (orders_df\n",
    "#                 .join(cdf_customers_df, [\"customer_id\"], \"inner\")\n",
    "#                 .writeStream\n",
    "#                     .foreachBatch(batch_upsert)\n",
    "#                     .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/customers_orders\")\n",
    "#                     .trigger(availableNow=True)\n",
    "#                     .start()\n",
    "#             )\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "\n",
    "# process_customers_orders()\n",
    "\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM customers_orders;\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "# process_customers_orders()\n",
    "\n",
    "# %sql\n",
    "# SELECT count(*) FROM customers_orders;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ff2b71-d8ee-4c07-8c7d-46dfae6c5928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def process_books_sales():\n",
    "    \n",
    "#     orders_df = (spark.readStream.table(\"orders_silver\")\n",
    "#                         .withColumn(\"book\", F.explode(\"books\"))\n",
    "#                 )\n",
    "\n",
    "#     books_df = spark.read.table(\"current_books\")\n",
    "\n",
    "#     query = (orders_df\n",
    "#                   .join(books_df, orders_df.book.book_id == books_df.book_id, \"inner\")\n",
    "#                   .writeStream\n",
    "#                      .outputMode(\"append\")\n",
    "#                      .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/books_sales\")\n",
    "#                      .trigger(availableNow=True)\n",
    "#                      .table(\"books_sales\")\n",
    "#     )\n",
    "\n",
    "#     query.awaitTermination()\n",
    "    \n",
    "# process_books_sales()\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM books_sales;\n",
    "# SELECT count(*) FROM books_sales;\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_books_silver()\n",
    "# bookstore.process_current_books()\n",
    "# process_books_sales()\n",
    "\n",
    "# %sql\n",
    "# SELECT count(*) FROM books_sales;\n",
    "\n",
    "# bookstore.process_orders_silver()\n",
    "# process_books_sales()\n",
    "\n",
    "# %sql\n",
    "# SELECT count(*) FROM books_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455a4a32-91cc-46e6-b0fd-7a9800c93909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE VIEW IF NOT EXISTS countries_stats_vw AS (\n",
    "#   SELECT country, date_trunc(\"DD\", order_timestamp) order_date, count(order_id) orders_count, sum(quantity) books_count\n",
    "#   FROM customers_orders\n",
    "#   GROUP BY country, date_trunc(\"DD\", order_timestamp)\n",
    "# );\n",
    "\n",
    "# -- execution is fast\n",
    "# SELECT *\n",
    "# FROM countries_stats_vw\n",
    "# WHERE country = \"France\";\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "# query = (spark.readStream\n",
    "#                  .table(\"books_sales\")\n",
    "#                  .withWatermark(\"order_timestamp\", \"10 minutes\")\n",
    "#                  .groupBy(\n",
    "#                      F.window(\"order_timestamp\", \"5 minutes\").alias(\"time\"),\n",
    "#                      \"author\")\n",
    "#                  .agg(\n",
    "#                      F.count(\"order_id\").alias(\"orders_count\"),\n",
    "#                      F.avg(\"quantity\").alias (\"avg_quantity\"))\n",
    "#               .writeStream\n",
    "#                  .option(\"checkpointLocation\", f\"dbfs:/mnt/demo_pro/checkpoints/authors_stats\")\n",
    "#                  .trigger(availableNow=True)\n",
    "#                  .table(\"authors_stats\")\n",
    "#             )\n",
    "# query.awaitTermination()\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM authors_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07cf651c-8817-4b58-858e-ce52151818be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8644864111829813,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
