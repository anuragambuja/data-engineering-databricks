{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d84644-8b99-4dc4-bbb2-0ad509ee5c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/derar-alhussein/Databricks-Certified-Data-Engineer-Professional/main/Includes/images/orders.png\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07bec64-664a-4a92-8c96-0f382abe5bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Copy-Datasets-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488f06a9-0bbe-4e06-80d6-3b4c88de3958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# files = dbutils.fs.ls(f\"{dataset_bookstore}/kafka-raw\")\n",
    "# display(files)\n",
    "\n",
    "# df_raw = spark.read.json(f\"{dataset_bookstore}/kafka-raw\")\n",
    "# display(df_raw)\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def process_bronze():\n",
    "  \n",
    "#     schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "#     query = (spark.readStream\n",
    "#                         .format(\"cloudFiles\")\n",
    "#                         .option(\"cloudFiles.format\", \"json\")\n",
    "#                         .schema(schema)\n",
    "#                         .load(f\"{dataset_bookstore}/kafka-raw\")\n",
    "#                         .withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))  \n",
    "#                         .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "#                   .writeStream\n",
    "#                       .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/bronze\")\n",
    "#                       .option(\"mergeSchema\", True) # enables schema evolution\n",
    "#                       .partitionBy(\"topic\", \"year_month\")\n",
    "#                       .trigger(availableNow=True)\n",
    "#                       .table(\"bronze\"))\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "\n",
    "# process_bronze()\n",
    "\n",
    "# batch_df = spark.table(\"bronze\")\n",
    "# display(batch_df)\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM bronze;\n",
    "# -- SELECT DISTINCT(topic) FROM bronze;\n",
    "# SELECT COUNT(*) FROM bronze\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# process_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb72a84-efe9-4752-9b68-58109541ac38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT cast(key AS STRING), cast(value AS STRING) FROM bronze LIMIT 20;\n",
    "\n",
    "# SELECT v.*\n",
    "# FROM (\n",
    "#   SELECT from_json(cast(value AS STRING), \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\") v\n",
    "#   FROM bronze\n",
    "#   WHERE topic = \"orders\")\n",
    "\n",
    "# (spark.readStream\n",
    "#       .table(\"bronze\")\n",
    "#       .createOrReplaceTempView(\"bronze_tmp\"))\n",
    "\n",
    "# %sql\n",
    "# CREATE OR REPLACE TEMPORARY VIEW orders_silver_tmp AS\n",
    "#   SELECT v.*\n",
    "#   FROM (\n",
    "#     SELECT from_json(cast(value AS STRING), \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\") v\n",
    "#     FROM bronze_tmp\n",
    "#     WHERE topic = \"orders\")\n",
    "\n",
    "# query = (spark.table(\"orders_silver_tmp\")\n",
    "#                .writeStream\n",
    "#                .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#                .trigger(availableNow=True)\n",
    "#                .table(\"orders_silver\"))\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "# query = (spark.readStream.table(\"bronze\")\n",
    "#         .filter(\"topic = 'orders'\")\n",
    "#         .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#         .select(\"v.*\")\n",
    "#      .writeStream\n",
    "#         .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#         .trigger(availableNow=True)\n",
    "#         .table(\"orders_silver\"))\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM orders_silver;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45da265-df20-48bf-8ec1-bc4dd4f1a91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE orders_silver ADD CONSTRAINT timestamp_within_range CHECK (order_timestamp >= '2020-01-01');\n",
    "# DESCRIBE EXTENDED orders_silver;\n",
    "\n",
    "# INSERT INTO orders_silver\n",
    "# VALUES ('1', '2022-02-01 00:00:00.000', 'C00001', 0, 0, NULL),\n",
    "#        ('2', '2019-05-01 00:00:00.000', 'C00001', 0, 0, NULL),\n",
    "#        ('3', '2023-01-01 00:00:00.000', 'C00001', 0, 0, NULL);\n",
    "\n",
    "# SELECT *\n",
    "# FROM orders_silver\n",
    "# WHERE order_id IN ('1', '2', '3');\n",
    "\n",
    "# ALTER TABLE orders_silver ADD CONSTRAINT valid_quantity CHECK (quantity > 0);\n",
    "\n",
    "# DESCRIBE EXTENDED orders_silver;\n",
    "\n",
    "# SELECT *\n",
    "# FROM orders_silver\n",
    "# where quantity <= 0;\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "# query = (spark.readStream.table(\"bronze\")\n",
    "#         .filter(\"topic = 'orders'\")\n",
    "#         .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#         .select(\"v.*\")\n",
    "#         .filter(\"quantity > 0\")\n",
    "#      .writeStream\n",
    "#         .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#         .trigger(availableNow=True)\n",
    "#         .table(\"orders_silver\"))\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# %sql\n",
    "# ALTER TABLE orders_silver DROP CONSTRAINT timestamp_within_range;\n",
    "# DESCRIBE EXTENDED orders_silver;\n",
    "# DROP TABLE orders_silver;\n",
    "\n",
    "# dbutils.fs.rm(\"dbfs:/mnt/demo_pro/checkpoints/orders_silver\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d41d8d9-2ad4-4b7b-9141-e6f5875f9a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (spark.read\n",
    "#       .table(\"bronze\")\n",
    "#       .filter(\"topic = 'orders'\")\n",
    "#       .count()\n",
    "# )\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "# batch_total = (spark.read\n",
    "#                       .table(\"bronze\")\n",
    "#                       .filter(\"topic = 'orders'\")\n",
    "#                       .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#                       .select(\"v.*\")\n",
    "#                       .dropDuplicates([\"order_id\", \"order_timestamp\"])\n",
    "#                       .count()\n",
    "#                 )\n",
    "\n",
    "# print(batch_total)\n",
    "\n",
    "# deduped_df = (spark.readStream\n",
    "#                    .table(\"bronze\")\n",
    "#                    .filter(\"topic = 'orders'\")\n",
    "#                    .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "#                    .select(\"v.*\")\n",
    "#                    .withWatermark(\"order_timestamp\", \"30 seconds\") ## ensures no duplicates in 30 secornd microbatch\n",
    "#                    .dropDuplicates([\"order_id\", \"order_timestamp\"]))\n",
    "\n",
    "# to make sure duplicates are not there in table\n",
    "# def upsert_data(microBatchDF, batch):\n",
    "#     microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "#     sql_query = \"\"\"\n",
    "#       MERGE INTO orders_silver a\n",
    "#       USING orders_microbatch b\n",
    "#       ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "#       WHEN NOT MATCHED THEN INSERT *\n",
    "#     \"\"\"\n",
    "    \n",
    "#     microBatchDF.sparkSession.sql(sql_query)\n",
    "#     #microBatchDF._jdf.sparkSession().sql(sql_query) # for runtime version below 10.5\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS orders_silver\n",
    "# (order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>);\n",
    "\n",
    "# query = (deduped_df.writeStream\n",
    "#                    .foreachBatch(upsert_data)\n",
    "#                    .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/orders_silver\")\n",
    "#                    .trigger(availableNow=True)\n",
    "#                    .start())\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "# streaming_total = spark.read.table(\"orders_silver\").count()\n",
    "# print(f\"batch total: {batch_total}\")\n",
    "# print(f\"streaming total: {streaming_total}\") # values match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67c7a5a-2d5f-454d-81b9-d7e0ef8bfa3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def type2_upsert(microBatchDF, batch):\n",
    "#     microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    \n",
    "#     sql_query = \"\"\"\n",
    "#         MERGE INTO books_silver\n",
    "#         USING (\n",
    "#             SELECT updates.book_id as merge_key, updates.*\n",
    "#             FROM updates\n",
    "\n",
    "#             UNION ALL\n",
    "\n",
    "#             SELECT NULL as merge_key, updates.*\n",
    "#             FROM updates\n",
    "#             JOIN books_silver ON updates.book_id = books_silver.book_id\n",
    "#             WHERE books_silver.current = true AND updates.price <> books_silver.price\n",
    "#           ) staged_updates\n",
    "#         ON books_silver.book_id = merge_key \n",
    "#         WHEN MATCHED AND books_silver.current = true AND books_silver.price <> staged_updates.price THEN\n",
    "#           UPDATE SET current = false, end_date = staged_updates.updated\n",
    "#         WHEN NOT MATCHED THEN\n",
    "#           INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "#           VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     microBatchDF.sparkSession.sql(sql_query)\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS books_silver\n",
    "# (book_id STRING, title STRING, author STRING, price DOUBLE, current BOOLEAN, effective_date TIMESTAMP, end_date TIMESTAMP)\n",
    "\n",
    "\n",
    "# def process_books():\n",
    "#     schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    " \n",
    "#     query = (spark.readStream\n",
    "#                     .table(\"bronze\")\n",
    "#                     .filter(\"topic = 'books'\")\n",
    "#                     .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                     .select(\"v.*\")\n",
    "#                  .writeStream\n",
    "#                     .foreachBatch(type2_upsert)\n",
    "#                     .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/books_silver\")\n",
    "#                     .trigger(availableNow=True)\n",
    "#                     .start()\n",
    "#             )\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "    \n",
    "# process_books()\n",
    "\n",
    "# books_df = spark.read.table(\"books_silver\").orderBy(\"book_id\", \"effective_date\")\n",
    "# display(books_df)\n",
    "\n",
    "# bookstore.load_books_updates()\n",
    "# bookstore.process_bronze()\n",
    "# process_books()\n",
    "\n",
    "# books_df = spark.read.table(\"books_silver\").orderBy(\"book_id\", \"effective_date\")\n",
    "# display(books_df)\n",
    "\n",
    "\n",
    "# %sql\n",
    "# CREATE OR REPLACE TABLE current_books\n",
    "# AS SELECT book_id, title, author, price\n",
    "#    FROM books_silver\n",
    "#    WHERE current IS TRUE;\n",
    "\n",
    "# SELECT *\n",
    "# FROM current_books\n",
    "# ORDER BY book_id;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc90297-a79c-4e0f-a202-153d375f6cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "# customers_df = (spark.table(\"bronze\")\n",
    "#                  .filter(\"topic = 'customers'\")\n",
    "#                  .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                  .select(\"v.*\")\n",
    "#                  .filter(F.col(\"row_status\").isin([\"insert\", \"update\"])))\n",
    "# display(customers_df)\n",
    "\n",
    "# from pyspark.sql.window import Window\n",
    "# window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "# ranked_df = (customers_df.withColumn(\"rank\", F.rank().over(window))\n",
    "#                           .filter(\"rank == 1\")\n",
    "#                           .drop(\"rank\"))\n",
    "# display(ranked_df)\n",
    "\n",
    "# This will throw an exception because non-time-based window operations are not supported on streaming DataFrames.\n",
    "# ranked_df = (spark.readStream\n",
    "#                    .table(\"bronze\")\n",
    "#                    .filter(\"topic = 'customers'\")\n",
    "#                    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                    .select(\"v.*\")\n",
    "#                    .filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "#                    .withColumn(\"rank\", F.rank().over(window))\n",
    "#                    .filter(\"rank == 1\")\n",
    "#                    .drop(\"rank\")\n",
    "#              )\n",
    "# display(ranked_df)\n",
    "\n",
    "\n",
    "# from pyspark.sql.window import Window\n",
    "# def batch_upsert(microBatchDF, batchId):\n",
    "#     window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "    \n",
    "#     (microBatchDF.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "#                  .withColumn(\"rank\", F.rank().over(window))\n",
    "#                  .filter(\"rank == 1\")\n",
    "#                  .drop(\"rank\")\n",
    "#                  .createOrReplaceTempView(\"ranked_updates\"))\n",
    "    \n",
    "#     query = \"\"\"\n",
    "#         MERGE INTO customers_silver c\n",
    "#         USING ranked_updates r\n",
    "#         ON c.customer_id=r.customer_id\n",
    "#             WHEN MATCHED AND c.row_time < r.row_time\n",
    "#               THEN UPDATE SET *\n",
    "#             WHEN NOT MATCHED\n",
    "#               THEN INSERT *\n",
    "#     \"\"\"\n",
    "#     microBatchDF.sparkSession.sql(query)\n",
    "\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS customers_silver\n",
    "# (customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country STRING, row_time TIMESTAMP)\n",
    "\n",
    "# df_country_lookup = spark.read.json(f\"{dataset_bookstore}/country_lookup\")\n",
    "# display(df_country_lookup)\n",
    "\n",
    "# query = (spark.readStream\n",
    "#                   .table(\"bronze\")\n",
    "#                   .filter(\"topic = 'customers'\")\n",
    "#                   .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                   .select(\"v.*\")\n",
    "#                   .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\") , \"inner\")\n",
    "#                .writeStream\n",
    "#                   .foreachBatch(batch_upsert)\n",
    "#                   .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/customers_silver\")\n",
    "#                   .trigger(availableNow=True)\n",
    "#                   .start()\n",
    "#           )\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# count = spark.table(\"customers_silver\").count()\n",
    "# expected_count = spark.table(\"customers_silver\").select(\"customer_id\").distinct().count()\n",
    "# assert count == expected_count\n",
    "# print(\"Unit test passed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0364fb5-3bf4-42ad-980a-3b2bd4630a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE customers_silver SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "# DESCRIBE TABLE EXTENDED customers_silver;\n",
    "# DESCRIBE HISTORY customers_silver;\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM table_changes(\"customers_silver\", 2);\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "\n",
    "# cdf_df = (spark.readStream\n",
    "#                .format(\"delta\")\n",
    "#                .option(\"readChangeData\", True)\n",
    "#                .option(\"startingVersion\", 2)\n",
    "#                .table(\"customers_silver\"))\n",
    "\n",
    "# display(cdf_df)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver\")\n",
    "# display(files)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver/_change_data\")\n",
    "# display(files)\n",
    "\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "\n",
    "# cdf_df = (spark.readStream\n",
    "#                .format(\"delta\")\n",
    "#                .option(\"readChangeData\", True)\n",
    "#                .option(\"startingVersion\", 2)\n",
    "#                .table(\"customers_silver\"))\n",
    "\n",
    "# display(cdf_df)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver\")\n",
    "# display(files)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/customers_silver/_change_data\")\n",
    "# display(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd9745b-4608-4fab-a38f-9c6e43a7a572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# def batch_upsert(microBatchDF, batchId):\n",
    "#     window = Window.partitionBy(\"order_id\", \"customer_id\").orderBy(F.col(\"_commit_timestamp\").desc())\n",
    "    \n",
    "#     (microBatchDF.filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\"]))\n",
    "#                  .withColumn(\"rank\", F.rank().over(window))\n",
    "#                  .filter(\"rank = 1\")\n",
    "#                  .drop(\"rank\", \"_change_type\", \"_commit_version\")\n",
    "#                  .withColumnRenamed(\"_commit_timestamp\", \"processed_timestamp\")\n",
    "#                  .createOrReplaceTempView(\"ranked_updates\"))\n",
    "    \n",
    "#     query = \"\"\"\n",
    "#         MERGE INTO customers_orders c\n",
    "#         USING ranked_updates r\n",
    "#         ON c.order_id=r.order_id AND c.customer_id=r.customer_id\n",
    "#             WHEN MATCHED AND c.processed_timestamp < r.processed_timestamp\n",
    "#               THEN UPDATE SET *\n",
    "#             WHEN NOT MATCHED\n",
    "#               THEN INSERT *\n",
    "#     \"\"\"\n",
    "    \n",
    "#     microBatchDF.sparkSession.sql(query)\n",
    "\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS customers_orders\n",
    "# (order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country STRING, row_time TIMESTAMP, processed_timestamp TIMESTAMP)\n",
    "\n",
    "\n",
    "# def process_customers_orders():\n",
    "#     orders_df = spark.readStream.table(\"orders_silver\")\n",
    "    \n",
    "#     cdf_customers_df = (spark.readStream\n",
    "#                              .option(\"readChangeData\", True)\n",
    "#                              .option(\"startingVersion\", 2)\n",
    "#                              .table(\"customers_silver\")\n",
    "#                        )\n",
    "\n",
    "#     query = (orders_df\n",
    "#                 .join(cdf_customers_df, [\"customer_id\"], \"inner\")\n",
    "#                 .writeStream\n",
    "#                     .foreachBatch(batch_upsert)\n",
    "#                     .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/customers_orders\")\n",
    "#                     .trigger(availableNow=True)\n",
    "#                     .start()\n",
    "#             )\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "\n",
    "# process_customers_orders()\n",
    "\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM customers_orders;\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_orders_silver()\n",
    "# bookstore.process_customers_silver()\n",
    "# process_customers_orders()\n",
    "\n",
    "# %sql\n",
    "# SELECT count(*) FROM customers_orders;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ff2b71-d8ee-4c07-8c7d-46dfae6c5928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def process_books_sales():\n",
    "    \n",
    "#     orders_df = (spark.readStream.table(\"orders_silver\")\n",
    "#                         .withColumn(\"book\", F.explode(\"books\"))\n",
    "#                 )\n",
    "\n",
    "#     books_df = spark.read.table(\"current_books\")\n",
    "\n",
    "#     query = (orders_df\n",
    "#                   .join(books_df, orders_df.book.book_id == books_df.book_id, \"inner\")\n",
    "#                   .writeStream\n",
    "#                      .outputMode(\"append\")\n",
    "#                      .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/books_sales\")\n",
    "#                      .trigger(availableNow=True)\n",
    "#                      .table(\"books_sales\")\n",
    "#     )\n",
    "\n",
    "#     query.awaitTermination()\n",
    "    \n",
    "# process_books_sales()\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM books_sales;\n",
    "# SELECT count(*) FROM books_sales;\n",
    "\n",
    "# bookstore.load_new_data()\n",
    "# bookstore.process_bronze()\n",
    "# bookstore.process_books_silver()\n",
    "# bookstore.process_current_books()\n",
    "# process_books_sales()\n",
    "\n",
    "# %sql\n",
    "# SELECT count(*) FROM books_sales;\n",
    "\n",
    "# bookstore.process_orders_silver()\n",
    "# process_books_sales()\n",
    "\n",
    "# %sql\n",
    "# SELECT count(*) FROM books_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455a4a32-91cc-46e6-b0fd-7a9800c93909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE VIEW IF NOT EXISTS countries_stats_vw AS (\n",
    "#   SELECT country, date_trunc(\"DD\", order_timestamp) order_date, count(order_id) orders_count, sum(quantity) books_count\n",
    "#   FROM customers_orders\n",
    "#   GROUP BY country, date_trunc(\"DD\", order_timestamp)\n",
    "# );\n",
    "\n",
    "# -- execution is fast\n",
    "# SELECT *\n",
    "# FROM countries_stats_vw\n",
    "# WHERE country = \"France\";\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "# query = (spark.readStream\n",
    "#                  .table(\"books_sales\")\n",
    "#                  .withWatermark(\"order_timestamp\", \"10 minutes\")\n",
    "#                  .groupBy(\n",
    "#                      F.window(\"order_timestamp\", \"5 minutes\").alias(\"time\"),\n",
    "#                      \"author\")\n",
    "#                  .agg(\n",
    "#                      F.count(\"order_id\").alias(\"orders_count\"),\n",
    "#                      F.avg(\"quantity\").alias (\"avg_quantity\"))\n",
    "#               .writeStream\n",
    "#                  .option(\"checkpointLocation\", f\"dbfs:/mnt/demo_pro/checkpoints/authors_stats\")\n",
    "#                  .trigger(availableNow=True)\n",
    "#                  .table(\"authors_stats\")\n",
    "#             )\n",
    "# query.awaitTermination()\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM authors_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07cf651c-8817-4b58-858e-ce52151818be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# DESCRIBE TABLE EXTENDED bronze;\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze\")\n",
    "# display(files)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze/topic=customers\")\n",
    "# display(files)\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze/topic=customers/year_month=2021-12/\")\n",
    "# display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478d01a3-052d-4748-aa1a-3b9867af9c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze/_delta_log\")\n",
    "# display(files)\n",
    "\n",
    "# statistics of data files are present in the add column for first 32 columns\n",
    "# display(spark.read.json(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze/_delta_log/00000000000000000001.json\")) \n",
    "\n",
    "# %sql\n",
    "# SELECT COUNT(*) FROM bronze;\n",
    "\n",
    "# files = dbutils.fs.ls(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze/_delta_log\")\n",
    "# display(files)\n",
    "\n",
    "# display(spark.read.parquet(\"dbfs:/user/hive/warehouse/bookstore_eng_pro.db/bronze/_delta_log/00000000000000000010.checkpoint.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e562cc-ae50-4cce-9a8f-6d93f03585aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## REST API\n",
    "# https://docs.databricks.com/api/workspace/introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368d7d2e-f86a-474f-a6aa-44feab002d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CLI #### command line / cmd in windows\n",
    "# https://docs.databricks.com/aws/en/dev-tools/cli\n",
    "\n",
    "# pip install databricks-cli\n",
    "# databricks configure --token   #### generate token from user settings in databricks\n",
    "# databricks -h\n",
    "# databricks clusters list\n",
    "# databricks clusters start --cluster-id <id>\n",
    "# databricks fs cp  dbfs:/mnt/uploads/test.json --overwrite\n",
    "# databricks secrets create-scope --scope bookstore-dev\n",
    "# databricks secrets put --scope bookstore-dev --key db_password --string-value 12345\n",
    "# databricks secrets list --scope bookstore-dev\n",
    "\n",
    "# %fs ls 'dbfs:/mnt/uploads'\n",
    "# dbutils.secrets.help()\n",
    "# db_password = dbutils.secrets.get(\"bookstore-dev\", \"db_password\")\n",
    "# print(db_password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a85abd8-d024-4d78-b8cb-83ebc0e63d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #### Propagating Deletes e.g. GDPR\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "\n",
    "# (spark.readStream\n",
    "#         .table(\"bronze\")\n",
    "#         .filter(\"topic = 'customers'\")\n",
    "#         .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#         .select(\"v.*\", F.col('v.row_time').alias(\"request_timestamp\"))\n",
    "#         .filter(\"row_status = 'delete'\")\n",
    "#         .select(\"customer_id\", \"request_timestamp\",\n",
    "#                 F.date_add(\"request_timestamp\", 30).alias(\"deadline\"), \n",
    "#                 F.lit(\"requested\").alias(\"status\"))\n",
    "#     .writeStream\n",
    "#         .outputMode(\"append\")\n",
    "#         .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/delete_requests\")\n",
    "#         .trigger(availableNow=True)\n",
    "#         .table(\"delete_requests\")\n",
    "# )\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM delete_requests;\n",
    "# DELETE FROM customers_silver WHERE customer_id IN (SELECT customer_id FROM delete_requests WHERE status = 'requested');\n",
    "\n",
    "# since the CDF was enabled\n",
    "# deleteDF = (spark.readStream\n",
    "#                  .format(\"delta\")\n",
    "#                  .option(\"readChangeFeed\", \"true\")\n",
    "#                  .option(\"startingVersion\", 2)\n",
    "#                  .table(\"customers_silver\"))\n",
    "\n",
    "# def process_deletes(microBatchDF, batchId):\n",
    "    \n",
    "#     (microBatchDF\n",
    "#         .filter(\"_change_type = 'delete'\")\n",
    "#         .createOrReplaceTempView(\"deletes\"))\n",
    "\n",
    "#     microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "#         DELETE FROM customers_orders\n",
    "#         WHERE customer_id IN (SELECT customer_id FROM deletes)\n",
    "#     \"\"\")\n",
    "    \n",
    "#     microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "#         MERGE INTO delete_requests r\n",
    "#         USING deletes d\n",
    "#         ON d.customer_id = r.customer_id\n",
    "#         WHEN MATCHED\n",
    "#           THEN UPDATE SET status = \"deleted\"\n",
    "#     \"\"\")\n",
    "\n",
    "# (deleteDF.writeStream\n",
    "#          .foreachBatch(process_deletes)\n",
    "#          .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/deletes\")\n",
    "#          .trigger(availableNow=True)\n",
    "#          .start())\n",
    "\n",
    "\n",
    "# %sql\n",
    "# SELECT * FROM delete_requests;\n",
    "# DESCRIBE HISTORY customers_orders;\n",
    "\n",
    "# ## Deleted records\n",
    "# SELECT * FROM customers_orders@v3\n",
    "# EXCEPT\n",
    "# SELECT * FROM customers_orders;\n",
    "\n",
    "# delete details present in CDF field \n",
    "# df = (spark.read\n",
    "#            .option(\"readChangeFeed\", \"true\")\n",
    "#            .option(\"startingVersion\", 2)\n",
    "#            .table(\"customers_silver\")\n",
    "#            .filter(\"_change_type = 'delete'\"))\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77584c53-b9f4-42de-9e43-d4fb6bdef005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- DESCRIBE TABLE customers_silver;\n",
    "\n",
    "-- CREATE OR REPLACE VIEW customers_vw AS\n",
    "--   SELECT\n",
    "--     customer_id,\n",
    "--     CASE \n",
    "--       WHEN is_member('admins_demo') THEN email\n",
    "--       ELSE 'REDACTED'\n",
    "--     END AS email,\n",
    "--     gender,\n",
    "--     CASE \n",
    "--       WHEN is_member('admins_demo') THEN first_name\n",
    "--       ELSE 'REDACTED'\n",
    "--     END AS first_name,\n",
    "--     CASE \n",
    "--       WHEN is_member('admins_demo') THEN last_name\n",
    "--       ELSE 'REDACTED'\n",
    "--     END AS last_name,\n",
    "--     CASE \n",
    "--       WHEN is_member('admins_demo') THEN street\n",
    "--       ELSE 'REDACTED'\n",
    "--     END AS street,\n",
    "--     city,\n",
    "--     country,\n",
    "--     row_time\n",
    "--   FROM customers_silver;\n",
    "\n",
    "-- SELECT * FROM customers_vw;\n",
    "\n",
    "-- ## Row Level Security\n",
    "-- CREATE OR REPLACE VIEW customers_fr_vw AS\n",
    "-- SELECT * FROM customers_vw\n",
    "-- WHERE \n",
    "--   CASE \n",
    "--     WHEN is_member('admins_demo') THEN TRUE\n",
    "--     ELSE country = \"France\" AND row_time > \"2022-01-01\"\n",
    "--   END;\n",
    "\n",
    "-- SELECT * FROM customers_fr_vw;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e082cd51-0194-4b02-97f3-61e0fd7b020f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################## Relative Imports ###############\n",
    "\n",
    "# %run ./helpers/cube_notebook\n",
    "\n",
    "# c1 = Cube(3)\n",
    "# c1.get_volume()\n",
    "\n",
    "# from helpers.cube_notebook import Cube ### dont work in notebook\n",
    "# from helpers.cube import Cube_PY\n",
    "# c2 = Cube_PY(3)\n",
    "# c2.get_volume()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29e41e6-e0bf-4773-898b-b12014b6c736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10259b2e-ba9e-4a18-836e-7cae09f3a661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh ls /databricks/driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f16b3fd-9777-4283-a3dd-16a12337dff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf44754f-6c81-4b90-92c2-974cb3a7cb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.append(os.path.abspath('./modules'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934aaa82-2bfa-42c5-9ffc-7f65659dc8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efee92e-5815-45ff-8759-6345035cbaf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from shapes.cube import Cube as CubeShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ec0352-d288-4dad-8c5f-8f868f3d33b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c3 = CubeShape(3)\n",
    "c3.get_volume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b803ed1d-bcd6-4052-ac79-f17dacd8da8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ./wheels/shapes-1.0.0-py3-none-any.whl  # install for all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e47d1b-0b4e-441c-bbd1-5a2349d35568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c4 = Cube_WHL(3)\n",
    "c4.get_volume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6805c9-8533-4f6c-84af-fbc073f7bd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh pip install ../wheels/shapes-1.0.0-py3-none-any.whl  # install in the driver machine only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8815f1fd-cd05-4103-9952-f5da551ef467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4611244519232969,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
